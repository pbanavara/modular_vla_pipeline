{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6089ad62-07e0-44af-900d-047c2ac0cfe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /Users/pbanavara/miniforge3/lib/python3.10/site-packages (2.3.1)\n",
      "Requirement already satisfied: torchvision in /Users/pbanavara/miniforge3/lib/python3.10/site-packages (0.18.1)\n",
      "Requirement already satisfied: torchaudio in /Users/pbanavara/miniforge3/lib/python3.10/site-packages (2.3.1)\n",
      "Requirement already satisfied: transformers in /Users/pbanavara/miniforge3/lib/python3.10/site-packages (4.49.0)\n",
      "Requirement already satisfied: diffusers in /Users/pbanavara/miniforge3/lib/python3.10/site-packages (0.29.2)\n",
      "Requirement already satisfied: accelerate in /Users/pbanavara/miniforge3/lib/python3.10/site-packages (1.5.1)\n",
      "Requirement already satisfied: safetensors in /Users/pbanavara/miniforge3/lib/python3.10/site-packages (0.4.3)\n",
      "Requirement already satisfied: filelock in /Users/pbanavara/miniforge3/lib/python3.10/site-packages (from torch) (3.15.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/pbanavara/miniforge3/lib/python3.10/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in /Users/pbanavara/miniforge3/lib/python3.10/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: networkx in /Users/pbanavara/miniforge3/lib/python3.10/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /Users/pbanavara/miniforge3/lib/python3.10/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /Users/pbanavara/miniforge3/lib/python3.10/site-packages (from torch) (2023.9.2)\n",
      "Requirement already satisfied: numpy in /Users/pbanavara/miniforge3/lib/python3.10/site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/pbanavara/miniforge3/lib/python3.10/site-packages (from torchvision) (10.4.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /Users/pbanavara/miniforge3/lib/python3.10/site-packages (from transformers) (0.29.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/pbanavara/miniforge3/lib/python3.10/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/pbanavara/miniforge3/lib/python3.10/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/pbanavara/miniforge3/lib/python3.10/site-packages (from transformers) (2024.5.15)\n",
      "Requirement already satisfied: requests in /Users/pbanavara/miniforge3/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /Users/pbanavara/miniforge3/lib/python3.10/site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/pbanavara/miniforge3/lib/python3.10/site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: importlib-metadata in /Users/pbanavara/miniforge3/lib/python3.10/site-packages (from diffusers) (8.0.0)\n",
      "Requirement already satisfied: psutil in /Users/pbanavara/miniforge3/lib/python3.10/site-packages (from accelerate) (6.0.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/pbanavara/miniforge3/lib/python3.10/site-packages (from importlib-metadata->diffusers) (3.19.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/pbanavara/miniforge3/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/pbanavara/miniforge3/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/pbanavara/miniforge3/lib/python3.10/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/pbanavara/miniforge3/lib/python3.10/site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/pbanavara/miniforge3/lib/python3.10/site-packages (from requests->transformers) (2024.12.14)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/pbanavara/miniforge3/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch torchvision torchaudio transformers diffusers accelerate safetensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "03bbfcc5-460e-4fbd-9afc-e5d20c893092",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: peft in /Users/pbanavara/miniforge3/lib/python3.10/site-packages (0.14.0)\n",
      "Requirement already satisfied: transformers in /Users/pbanavara/miniforge3/lib/python3.10/site-packages (4.43.0.dev0)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.49.0-py3-none-any.whl.metadata (44 kB)\n",
      "Requirement already satisfied: accelerate in /Users/pbanavara/miniforge3/lib/python3.10/site-packages (0.32.1)\n",
      "Collecting accelerate\n",
      "  Downloading accelerate-1.5.1-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/pbanavara/miniforge3/lib/python3.10/site-packages (from peft) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/pbanavara/miniforge3/lib/python3.10/site-packages (from peft) (24.1)\n",
      "Requirement already satisfied: psutil in /Users/pbanavara/miniforge3/lib/python3.10/site-packages (from peft) (6.0.0)\n",
      "Requirement already satisfied: pyyaml in /Users/pbanavara/miniforge3/lib/python3.10/site-packages (from peft) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.13.0 in /Users/pbanavara/miniforge3/lib/python3.10/site-packages (from peft) (2.3.1)\n",
      "Requirement already satisfied: tqdm in /Users/pbanavara/miniforge3/lib/python3.10/site-packages (from peft) (4.66.4)\n",
      "Requirement already satisfied: safetensors in /Users/pbanavara/miniforge3/lib/python3.10/site-packages (from peft) (0.4.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.25.0 in /Users/pbanavara/miniforge3/lib/python3.10/site-packages (from peft) (0.29.3)\n",
      "Requirement already satisfied: filelock in /Users/pbanavara/miniforge3/lib/python3.10/site-packages (from transformers) (3.15.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/pbanavara/miniforge3/lib/python3.10/site-packages (from transformers) (2024.5.15)\n",
      "Requirement already satisfied: requests in /Users/pbanavara/miniforge3/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Using cached tokenizers-0.21.0-cp39-abi3-macosx_11_0_arm64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/pbanavara/miniforge3/lib/python3.10/site-packages (from huggingface-hub>=0.25.0->peft) (2023.9.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/pbanavara/miniforge3/lib/python3.10/site-packages (from huggingface-hub>=0.25.0->peft) (4.12.2)\n",
      "Requirement already satisfied: sympy in /Users/pbanavara/miniforge3/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.13.1)\n",
      "Requirement already satisfied: networkx in /Users/pbanavara/miniforge3/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.3)\n",
      "Requirement already satisfied: jinja2 in /Users/pbanavara/miniforge3/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/pbanavara/miniforge3/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/pbanavara/miniforge3/lib/python3.10/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/pbanavara/miniforge3/lib/python3.10/site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/pbanavara/miniforge3/lib/python3.10/site-packages (from requests->transformers) (2024.12.14)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/pbanavara/miniforge3/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/pbanavara/miniforge3/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n",
      "Downloading transformers-4.49.0-py3-none-any.whl (10.0 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m35.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mMB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading accelerate-1.5.1-py3-none-any.whl (345 kB)\n",
      "Using cached tokenizers-0.21.0-cp39-abi3-macosx_11_0_arm64.whl (2.6 MB)\n",
      "Installing collected packages: tokenizers, accelerate, transformers\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.19.1\n",
      "    Uninstalling tokenizers-0.19.1:\n",
      "      Successfully uninstalled tokenizers-0.19.1\n",
      "  Attempting uninstall: accelerate\n",
      "    Found existing installation: accelerate 0.32.1\n",
      "    Uninstalling accelerate-0.32.1:\n",
      "      Successfully uninstalled accelerate-0.32.1\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.43.0.dev0\n",
      "    Uninstalling transformers-4.43.0.dev0:\n",
      "      Successfully uninstalled transformers-4.43.0.dev0\n",
      "Successfully installed accelerate-1.5.1 tokenizers-0.21.0 transformers-4.49.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install peft transformers accelerate --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "48692156-2be8-4dde-9cbc-4f5dc96d7403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 LoRA Weights Contain These Layers:\n",
      "\n",
      "['base_model.model.down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_k.lora_A.weight', 'base_model.model.down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_k.lora_B.weight', 'base_model.model.down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_q.lora_A.weight', 'base_model.model.down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_q.lora_B.weight', 'base_model.model.down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_k.lora_A.weight', 'base_model.model.down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_k.lora_B.weight', 'base_model.model.down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_q.lora_A.weight', 'base_model.model.down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_q.lora_B.weight', 'base_model.model.down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_k.lora_A.weight', 'base_model.model.down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_k.lora_B.weight', 'base_model.model.down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_q.lora_A.weight', 'base_model.model.down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_q.lora_B.weight', 'base_model.model.down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_k.lora_A.weight', 'base_model.model.down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_k.lora_B.weight', 'base_model.model.down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_q.lora_A.weight', 'base_model.model.down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_q.lora_B.weight', 'base_model.model.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_k.lora_A.weight', 'base_model.model.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_k.lora_B.weight', 'base_model.model.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_q.lora_A.weight', 'base_model.model.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_q.lora_B.weight', 'base_model.model.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k.lora_A.weight', 'base_model.model.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k.lora_B.weight', 'base_model.model.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_q.lora_A.weight', 'base_model.model.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_q.lora_B.weight', 'base_model.model.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_k.lora_A.weight', 'base_model.model.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_k.lora_B.weight', 'base_model.model.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_q.lora_A.weight', 'base_model.model.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_q.lora_B.weight', 'base_model.model.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k.lora_A.weight', 'base_model.model.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k.lora_B.weight', 'base_model.model.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_q.lora_A.weight', 'base_model.model.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_q.lora_B.weight', 'base_model.model.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_k.lora_A.weight', 'base_model.model.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_k.lora_B.weight', 'base_model.model.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_q.lora_A.weight', 'base_model.model.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_q.lora_B.weight', 'base_model.model.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_k.lora_A.weight', 'base_model.model.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_k.lora_B.weight', 'base_model.model.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_q.lora_A.weight', 'base_model.model.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_q.lora_B.weight', 'base_model.model.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_k.lora_A.weight', 'base_model.model.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_k.lora_B.weight', 'base_model.model.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_q.lora_A.weight', 'base_model.model.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_q.lora_B.weight', 'base_model.model.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_k.lora_A.weight', 'base_model.model.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_k.lora_B.weight', 'base_model.model.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_q.lora_A.weight', 'base_model.model.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_q.lora_B.weight', 'base_model.model.mid_block.attentions.0.transformer_blocks.0.attn1.to_k.lora_A.weight', 'base_model.model.mid_block.attentions.0.transformer_blocks.0.attn1.to_k.lora_B.weight']\n"
     ]
    }
   ],
   "source": [
    "from safetensors.torch import load_file\n",
    "\n",
    "# Load LoRA weights using safetensors\n",
    "state_dict = load_file(\"weights/adapter_model.safetensors\")\n",
    "\n",
    "# Print all layer names\n",
    "print(\"\\n🔹 LoRA Weights Contain These Layers:\\n\")\n",
    "print(list(state_dict.keys())[:50])  # Show first 50 keys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1f6d5cb7-881a-48d0-8eec-f920b24ad414",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6b70f9d5d7a4ab19fb36084f4b579d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Base Model UNet Layers:\n",
      "\n",
      "\n",
      "conv_in\n",
      "time_proj\n",
      "time_embedding\n",
      "time_embedding.linear_1\n",
      "time_embedding.act\n",
      "time_embedding.linear_2\n",
      "down_blocks\n",
      "down_blocks.0\n",
      "down_blocks.0.attentions\n",
      "down_blocks.0.attentions.0\n",
      "down_blocks.0.attentions.0.norm\n",
      "down_blocks.0.attentions.0.proj_in\n",
      "down_blocks.0.attentions.0.transformer_blocks\n",
      "down_blocks.0.attentions.0.transformer_blocks.0\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.norm1\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.attn1\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_q\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_k\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_v\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_out\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_out.0\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_out.1\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.norm2\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.attn2\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_q\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_k\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_v\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_out\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_out.0\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_out.1\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.norm3\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.ff\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.ff.net\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.ff.net.0\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.ff.net.0.proj\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.ff.net.1\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.ff.net.2\n",
      "down_blocks.0.attentions.0.proj_out\n",
      "down_blocks.0.attentions.1\n",
      "down_blocks.0.attentions.1.norm\n",
      "down_blocks.0.attentions.1.proj_in\n",
      "down_blocks.0.attentions.1.transformer_blocks\n",
      "down_blocks.0.attentions.1.transformer_blocks.0\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.norm1\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.attn1\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_q\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_k\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_v\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_out\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_out.0\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_out.1\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.norm2\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.attn2\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_q\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_k\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_v\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_out\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_out.0\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_out.1\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.norm3\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.ff\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.ff.net\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.ff.net.0\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.ff.net.0.proj\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.ff.net.1\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.ff.net.2\n",
      "down_blocks.0.attentions.1.proj_out\n",
      "down_blocks.0.resnets\n",
      "down_blocks.0.resnets.0\n",
      "down_blocks.0.resnets.0.norm1\n",
      "down_blocks.0.resnets.0.conv1\n",
      "down_blocks.0.resnets.0.time_emb_proj\n",
      "down_blocks.0.resnets.0.norm2\n",
      "down_blocks.0.resnets.0.dropout\n",
      "down_blocks.0.resnets.0.conv2\n",
      "down_blocks.0.resnets.1\n",
      "down_blocks.0.resnets.1.norm1\n",
      "down_blocks.0.resnets.1.conv1\n",
      "down_blocks.0.resnets.1.time_emb_proj\n",
      "down_blocks.0.resnets.1.norm2\n",
      "down_blocks.0.resnets.1.dropout\n",
      "down_blocks.0.resnets.1.conv2\n",
      "down_blocks.0.downsamplers\n",
      "down_blocks.0.downsamplers.0\n",
      "down_blocks.0.downsamplers.0.conv\n",
      "down_blocks.1\n",
      "down_blocks.1.attentions\n",
      "down_blocks.1.attentions.0\n",
      "down_blocks.1.attentions.0.norm\n",
      "down_blocks.1.attentions.0.proj_in\n",
      "down_blocks.1.attentions.0.transformer_blocks\n",
      "down_blocks.1.attentions.0.transformer_blocks.0\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.norm1\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.attn1\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_q\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_k\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_v\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.1\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.norm2\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.attn2\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_q\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.1\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.norm3\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.ff\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.ff.net\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.ff.net.0\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.ff.net.1\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.ff.net.2\n",
      "down_blocks.1.attentions.0.proj_out\n",
      "down_blocks.1.attentions.1\n",
      "down_blocks.1.attentions.1.norm\n",
      "down_blocks.1.attentions.1.proj_in\n",
      "down_blocks.1.attentions.1.transformer_blocks\n",
      "down_blocks.1.attentions.1.transformer_blocks.0\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.norm1\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.attn1\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_q\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_k\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_v\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.1\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.norm2\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.attn2\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_q\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_v\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.1\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.norm3\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.ff\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.ff.net\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.ff.net.0\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.ff.net.1\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.ff.net.2\n",
      "down_blocks.1.attentions.1.proj_out\n",
      "down_blocks.1.resnets\n",
      "down_blocks.1.resnets.0\n",
      "down_blocks.1.resnets.0.norm1\n",
      "down_blocks.1.resnets.0.conv1\n",
      "down_blocks.1.resnets.0.time_emb_proj\n",
      "down_blocks.1.resnets.0.norm2\n",
      "down_blocks.1.resnets.0.dropout\n",
      "down_blocks.1.resnets.0.conv2\n",
      "down_blocks.1.resnets.0.conv_shortcut\n",
      "down_blocks.1.resnets.1\n",
      "down_blocks.1.resnets.1.norm1\n",
      "down_blocks.1.resnets.1.conv1\n",
      "down_blocks.1.resnets.1.time_emb_proj\n",
      "down_blocks.1.resnets.1.norm2\n",
      "down_blocks.1.resnets.1.dropout\n",
      "down_blocks.1.resnets.1.conv2\n",
      "down_blocks.1.downsamplers\n",
      "down_blocks.1.downsamplers.0\n",
      "down_blocks.1.downsamplers.0.conv\n",
      "down_blocks.2\n",
      "down_blocks.2.attentions\n",
      "down_blocks.2.attentions.0\n",
      "down_blocks.2.attentions.0.norm\n",
      "down_blocks.2.attentions.0.proj_in\n",
      "down_blocks.2.attentions.0.transformer_blocks\n",
      "down_blocks.2.attentions.0.transformer_blocks.0\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.norm1\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.attn1\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_q\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_k\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_v\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.1\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.norm2\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.attn2\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_q\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_k\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_v\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.1\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.norm3\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.ff\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.ff.net\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.ff.net.0\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.ff.net.1\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.ff.net.2\n",
      "down_blocks.2.attentions.0.proj_out\n",
      "down_blocks.2.attentions.1\n",
      "down_blocks.2.attentions.1.norm\n",
      "down_blocks.2.attentions.1.proj_in\n",
      "down_blocks.2.attentions.1.transformer_blocks\n",
      "down_blocks.2.attentions.1.transformer_blocks.0\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.norm1\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.attn1\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_q\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_k\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_v\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.1\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.norm2\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.attn2\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_q\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_k\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_v\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.1\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.norm3\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.ff\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.ff.net\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.ff.net.0\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.ff.net.1\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.ff.net.2\n",
      "down_blocks.2.attentions.1.proj_out\n",
      "down_blocks.2.resnets\n",
      "down_blocks.2.resnets.0\n",
      "down_blocks.2.resnets.0.norm1\n",
      "down_blocks.2.resnets.0.conv1\n",
      "down_blocks.2.resnets.0.time_emb_proj\n",
      "down_blocks.2.resnets.0.norm2\n",
      "down_blocks.2.resnets.0.dropout\n",
      "down_blocks.2.resnets.0.conv2\n",
      "down_blocks.2.resnets.0.conv_shortcut\n",
      "down_blocks.2.resnets.1\n",
      "down_blocks.2.resnets.1.norm1\n",
      "down_blocks.2.resnets.1.conv1\n",
      "down_blocks.2.resnets.1.time_emb_proj\n",
      "down_blocks.2.resnets.1.norm2\n",
      "down_blocks.2.resnets.1.dropout\n",
      "down_blocks.2.resnets.1.conv2\n",
      "down_blocks.2.downsamplers\n",
      "down_blocks.2.downsamplers.0\n",
      "down_blocks.2.downsamplers.0.conv\n",
      "down_blocks.3\n",
      "down_blocks.3.resnets\n",
      "down_blocks.3.resnets.0\n",
      "down_blocks.3.resnets.0.norm1\n",
      "down_blocks.3.resnets.0.conv1\n",
      "down_blocks.3.resnets.0.time_emb_proj\n",
      "down_blocks.3.resnets.0.norm2\n",
      "down_blocks.3.resnets.0.dropout\n",
      "down_blocks.3.resnets.0.conv2\n",
      "down_blocks.3.resnets.1\n",
      "down_blocks.3.resnets.1.norm1\n",
      "down_blocks.3.resnets.1.conv1\n",
      "down_blocks.3.resnets.1.time_emb_proj\n",
      "down_blocks.3.resnets.1.norm2\n",
      "down_blocks.3.resnets.1.dropout\n",
      "down_blocks.3.resnets.1.conv2\n",
      "up_blocks\n",
      "up_blocks.0\n",
      "up_blocks.0.resnets\n",
      "up_blocks.0.resnets.0\n",
      "up_blocks.0.resnets.0.norm1\n",
      "up_blocks.0.resnets.0.conv1\n",
      "up_blocks.0.resnets.0.time_emb_proj\n",
      "up_blocks.0.resnets.0.norm2\n",
      "up_blocks.0.resnets.0.dropout\n",
      "up_blocks.0.resnets.0.conv2\n",
      "up_blocks.0.resnets.0.conv_shortcut\n",
      "up_blocks.0.resnets.1\n",
      "up_blocks.0.resnets.1.norm1\n",
      "up_blocks.0.resnets.1.conv1\n",
      "up_blocks.0.resnets.1.time_emb_proj\n",
      "up_blocks.0.resnets.1.norm2\n",
      "up_blocks.0.resnets.1.dropout\n",
      "up_blocks.0.resnets.1.conv2\n",
      "up_blocks.0.resnets.1.conv_shortcut\n",
      "up_blocks.0.resnets.2\n",
      "up_blocks.0.resnets.2.norm1\n",
      "up_blocks.0.resnets.2.conv1\n",
      "up_blocks.0.resnets.2.time_emb_proj\n",
      "up_blocks.0.resnets.2.norm2\n",
      "up_blocks.0.resnets.2.dropout\n",
      "up_blocks.0.resnets.2.conv2\n",
      "up_blocks.0.resnets.2.conv_shortcut\n",
      "up_blocks.0.upsamplers\n",
      "up_blocks.0.upsamplers.0\n",
      "up_blocks.0.upsamplers.0.conv\n",
      "up_blocks.1\n",
      "up_blocks.1.attentions\n",
      "up_blocks.1.attentions.0\n",
      "up_blocks.1.attentions.0.norm\n",
      "up_blocks.1.attentions.0.proj_in\n",
      "up_blocks.1.attentions.0.transformer_blocks\n",
      "up_blocks.1.attentions.0.transformer_blocks.0\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.norm1\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.attn1\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_q\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_k\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_v\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.1\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.norm2\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.attn2\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_q\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.1\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.norm3\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.ff\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.ff.net\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.ff.net.0\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.ff.net.1\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.ff.net.2\n",
      "up_blocks.1.attentions.0.proj_out\n",
      "up_blocks.1.attentions.1\n",
      "up_blocks.1.attentions.1.norm\n",
      "up_blocks.1.attentions.1.proj_in\n",
      "up_blocks.1.attentions.1.transformer_blocks\n",
      "up_blocks.1.attentions.1.transformer_blocks.0\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.norm1\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.attn1\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_q\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_k\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_v\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.1\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.norm2\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.attn2\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_q\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_v\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.1\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.norm3\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.ff\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.ff.net\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.ff.net.0\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.ff.net.1\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.ff.net.2\n",
      "up_blocks.1.attentions.1.proj_out\n",
      "up_blocks.1.attentions.2\n",
      "up_blocks.1.attentions.2.norm\n",
      "up_blocks.1.attentions.2.proj_in\n",
      "up_blocks.1.attentions.2.transformer_blocks\n",
      "up_blocks.1.attentions.2.transformer_blocks.0\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.norm1\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.attn1\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_q\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_k\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_v\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_out\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_out.0\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_out.1\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.norm2\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.attn2\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_q\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_k\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_v\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_out\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_out.0\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_out.1\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.norm3\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.ff\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.ff.net\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.ff.net.0\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.ff.net.0.proj\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.ff.net.1\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.ff.net.2\n",
      "up_blocks.1.attentions.2.proj_out\n",
      "up_blocks.1.resnets\n",
      "up_blocks.1.resnets.0\n",
      "up_blocks.1.resnets.0.norm1\n",
      "up_blocks.1.resnets.0.conv1\n",
      "up_blocks.1.resnets.0.time_emb_proj\n",
      "up_blocks.1.resnets.0.norm2\n",
      "up_blocks.1.resnets.0.dropout\n",
      "up_blocks.1.resnets.0.conv2\n",
      "up_blocks.1.resnets.0.conv_shortcut\n",
      "up_blocks.1.resnets.1\n",
      "up_blocks.1.resnets.1.norm1\n",
      "up_blocks.1.resnets.1.conv1\n",
      "up_blocks.1.resnets.1.time_emb_proj\n",
      "up_blocks.1.resnets.1.norm2\n",
      "up_blocks.1.resnets.1.dropout\n",
      "up_blocks.1.resnets.1.conv2\n",
      "up_blocks.1.resnets.1.conv_shortcut\n",
      "up_blocks.1.resnets.2\n",
      "up_blocks.1.resnets.2.norm1\n",
      "up_blocks.1.resnets.2.conv1\n",
      "up_blocks.1.resnets.2.time_emb_proj\n",
      "up_blocks.1.resnets.2.norm2\n",
      "up_blocks.1.resnets.2.dropout\n",
      "up_blocks.1.resnets.2.conv2\n",
      "up_blocks.1.resnets.2.conv_shortcut\n",
      "up_blocks.1.upsamplers\n",
      "up_blocks.1.upsamplers.0\n",
      "up_blocks.1.upsamplers.0.conv\n",
      "up_blocks.2\n",
      "up_blocks.2.attentions\n",
      "up_blocks.2.attentions.0\n",
      "up_blocks.2.attentions.0.norm\n",
      "up_blocks.2.attentions.0.proj_in\n",
      "up_blocks.2.attentions.0.transformer_blocks\n",
      "up_blocks.2.attentions.0.transformer_blocks.0\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.norm1\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.attn1\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_q\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_k\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_v\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.1\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.norm2\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.attn2\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_q\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_k\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_v\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.1\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.norm3\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.ff\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.ff.net\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.ff.net.0\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.ff.net.1\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.ff.net.2\n",
      "up_blocks.2.attentions.0.proj_out\n",
      "up_blocks.2.attentions.1\n",
      "up_blocks.2.attentions.1.norm\n",
      "up_blocks.2.attentions.1.proj_in\n",
      "up_blocks.2.attentions.1.transformer_blocks\n",
      "up_blocks.2.attentions.1.transformer_blocks.0\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.norm1\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.attn1\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_q\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_k\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_v\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.1\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.norm2\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.attn2\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_q\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_k\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_v\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.1\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.norm3\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.ff\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.ff.net\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.ff.net.0\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.ff.net.1\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.ff.net.2\n",
      "up_blocks.2.attentions.1.proj_out\n",
      "up_blocks.2.attentions.2\n",
      "up_blocks.2.attentions.2.norm\n",
      "up_blocks.2.attentions.2.proj_in\n",
      "up_blocks.2.attentions.2.transformer_blocks\n",
      "up_blocks.2.attentions.2.transformer_blocks.0\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.norm1\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.attn1\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_q\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_k\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_v\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_out\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_out.0\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_out.1\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.norm2\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.attn2\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_q\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_k\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_v\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_out\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_out.0\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_out.1\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.norm3\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.ff\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.ff.net\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.ff.net.0\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.ff.net.0.proj\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.ff.net.1\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.ff.net.2\n",
      "up_blocks.2.attentions.2.proj_out\n",
      "up_blocks.2.resnets\n",
      "up_blocks.2.resnets.0\n",
      "up_blocks.2.resnets.0.norm1\n",
      "up_blocks.2.resnets.0.conv1\n",
      "up_blocks.2.resnets.0.time_emb_proj\n",
      "up_blocks.2.resnets.0.norm2\n",
      "up_blocks.2.resnets.0.dropout\n",
      "up_blocks.2.resnets.0.conv2\n",
      "up_blocks.2.resnets.0.conv_shortcut\n",
      "up_blocks.2.resnets.1\n",
      "up_blocks.2.resnets.1.norm1\n",
      "up_blocks.2.resnets.1.conv1\n",
      "up_blocks.2.resnets.1.time_emb_proj\n",
      "up_blocks.2.resnets.1.norm2\n",
      "up_blocks.2.resnets.1.dropout\n",
      "up_blocks.2.resnets.1.conv2\n",
      "up_blocks.2.resnets.1.conv_shortcut\n",
      "up_blocks.2.resnets.2\n",
      "up_blocks.2.resnets.2.norm1\n",
      "up_blocks.2.resnets.2.conv1\n",
      "up_blocks.2.resnets.2.time_emb_proj\n",
      "up_blocks.2.resnets.2.norm2\n",
      "up_blocks.2.resnets.2.dropout\n",
      "up_blocks.2.resnets.2.conv2\n",
      "up_blocks.2.resnets.2.conv_shortcut\n",
      "up_blocks.2.upsamplers\n",
      "up_blocks.2.upsamplers.0\n",
      "up_blocks.2.upsamplers.0.conv\n",
      "up_blocks.3\n",
      "up_blocks.3.attentions\n",
      "up_blocks.3.attentions.0\n",
      "up_blocks.3.attentions.0.norm\n",
      "up_blocks.3.attentions.0.proj_in\n",
      "up_blocks.3.attentions.0.transformer_blocks\n",
      "up_blocks.3.attentions.0.transformer_blocks.0\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.norm1\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.attn1\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_q\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_k\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_v\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_out\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_out.0\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_out.1\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.norm2\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.attn2\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_q\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_k\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_v\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_out\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_out.0\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_out.1\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.norm3\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.ff\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.ff.net\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.ff.net.0\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.ff.net.0.proj\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.ff.net.1\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.ff.net.2\n",
      "up_blocks.3.attentions.0.proj_out\n",
      "up_blocks.3.attentions.1\n",
      "up_blocks.3.attentions.1.norm\n",
      "up_blocks.3.attentions.1.proj_in\n",
      "up_blocks.3.attentions.1.transformer_blocks\n",
      "up_blocks.3.attentions.1.transformer_blocks.0\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.norm1\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.attn1\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_q\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_k\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_v\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_out\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_out.0\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_out.1\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.norm2\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.attn2\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_q\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_k\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_v\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_out\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_out.0\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_out.1\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.norm3\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.ff\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.ff.net\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.ff.net.0\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.ff.net.0.proj\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.ff.net.1\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.ff.net.2\n",
      "up_blocks.3.attentions.1.proj_out\n",
      "up_blocks.3.attentions.2\n",
      "up_blocks.3.attentions.2.norm\n",
      "up_blocks.3.attentions.2.proj_in\n",
      "up_blocks.3.attentions.2.transformer_blocks\n",
      "up_blocks.3.attentions.2.transformer_blocks.0\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.norm1\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.attn1\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_q\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_k\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_v\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_out\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_out.0\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_out.1\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.norm2\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.attn2\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_q\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_k\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_v\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_out\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_out.0\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_out.1\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.norm3\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.ff\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.ff.net\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.ff.net.0\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.ff.net.0.proj\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.ff.net.1\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.ff.net.2\n",
      "up_blocks.3.attentions.2.proj_out\n",
      "up_blocks.3.resnets\n",
      "up_blocks.3.resnets.0\n",
      "up_blocks.3.resnets.0.norm1\n",
      "up_blocks.3.resnets.0.conv1\n",
      "up_blocks.3.resnets.0.time_emb_proj\n",
      "up_blocks.3.resnets.0.norm2\n",
      "up_blocks.3.resnets.0.dropout\n",
      "up_blocks.3.resnets.0.conv2\n",
      "up_blocks.3.resnets.0.conv_shortcut\n",
      "up_blocks.3.resnets.1\n",
      "up_blocks.3.resnets.1.norm1\n",
      "up_blocks.3.resnets.1.conv1\n",
      "up_blocks.3.resnets.1.time_emb_proj\n",
      "up_blocks.3.resnets.1.norm2\n",
      "up_blocks.3.resnets.1.dropout\n",
      "up_blocks.3.resnets.1.conv2\n",
      "up_blocks.3.resnets.1.conv_shortcut\n",
      "up_blocks.3.resnets.2\n",
      "up_blocks.3.resnets.2.norm1\n",
      "up_blocks.3.resnets.2.conv1\n",
      "up_blocks.3.resnets.2.time_emb_proj\n",
      "up_blocks.3.resnets.2.norm2\n",
      "up_blocks.3.resnets.2.dropout\n",
      "up_blocks.3.resnets.2.conv2\n",
      "up_blocks.3.resnets.2.conv_shortcut\n",
      "mid_block\n",
      "mid_block.attentions\n",
      "mid_block.attentions.0\n",
      "mid_block.attentions.0.norm\n",
      "mid_block.attentions.0.proj_in\n",
      "mid_block.attentions.0.transformer_blocks\n",
      "mid_block.attentions.0.transformer_blocks.0\n",
      "mid_block.attentions.0.transformer_blocks.0.norm1\n",
      "mid_block.attentions.0.transformer_blocks.0.attn1\n",
      "mid_block.attentions.0.transformer_blocks.0.attn1.to_q\n",
      "mid_block.attentions.0.transformer_blocks.0.attn1.to_k\n",
      "mid_block.attentions.0.transformer_blocks.0.attn1.to_v\n",
      "mid_block.attentions.0.transformer_blocks.0.attn1.to_out\n",
      "mid_block.attentions.0.transformer_blocks.0.attn1.to_out.0\n",
      "mid_block.attentions.0.transformer_blocks.0.attn1.to_out.1\n",
      "mid_block.attentions.0.transformer_blocks.0.norm2\n",
      "mid_block.attentions.0.transformer_blocks.0.attn2\n",
      "mid_block.attentions.0.transformer_blocks.0.attn2.to_q\n",
      "mid_block.attentions.0.transformer_blocks.0.attn2.to_k\n",
      "mid_block.attentions.0.transformer_blocks.0.attn2.to_v\n",
      "mid_block.attentions.0.transformer_blocks.0.attn2.to_out\n",
      "mid_block.attentions.0.transformer_blocks.0.attn2.to_out.0\n",
      "mid_block.attentions.0.transformer_blocks.0.attn2.to_out.1\n",
      "mid_block.attentions.0.transformer_blocks.0.norm3\n",
      "mid_block.attentions.0.transformer_blocks.0.ff\n",
      "mid_block.attentions.0.transformer_blocks.0.ff.net\n",
      "mid_block.attentions.0.transformer_blocks.0.ff.net.0\n",
      "mid_block.attentions.0.transformer_blocks.0.ff.net.0.proj\n",
      "mid_block.attentions.0.transformer_blocks.0.ff.net.1\n",
      "mid_block.attentions.0.transformer_blocks.0.ff.net.2\n",
      "mid_block.attentions.0.proj_out\n",
      "mid_block.resnets\n",
      "mid_block.resnets.0\n",
      "mid_block.resnets.0.norm1\n",
      "mid_block.resnets.0.conv1\n",
      "mid_block.resnets.0.time_emb_proj\n",
      "mid_block.resnets.0.norm2\n",
      "mid_block.resnets.0.dropout\n",
      "mid_block.resnets.0.conv2\n",
      "mid_block.resnets.1\n",
      "mid_block.resnets.1.norm1\n",
      "mid_block.resnets.1.conv1\n",
      "mid_block.resnets.1.time_emb_proj\n",
      "mid_block.resnets.1.norm2\n",
      "mid_block.resnets.1.dropout\n",
      "mid_block.resnets.1.conv2\n",
      "conv_norm_out\n",
      "conv_out\n"
     ]
    }
   ],
   "source": [
    "# check base model UNET layers\n",
    "# ✅ Print all available UNet layers\n",
    "from diffusers import StableDiffusionPipeline\n",
    "\n",
    "# Load Base Model\n",
    "base_model = \"CompVis/stable-diffusion-v1-4\"\n",
    "pipe_old = StableDiffusionPipeline.from_pretrained(base_model, torch_dtype=torch.float16)\n",
    "\n",
    "# ✅ Print all available UNet layers\n",
    "print(\"\\n🔹 Base Model UNet Layers:\\n\")\n",
    "for name, _ in pipe_old.unet.named_modules():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f0f1c995-d361-49a3-b456-8e489dc7e286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 LoRA Weights Contain These Layers:\n",
      "\n",
      "base_model.model.down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_k.lora_A.weight\n",
      "base_model.model.down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_k.lora_B.weight\n",
      "base_model.model.down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_q.lora_A.weight\n",
      "base_model.model.down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_q.lora_B.weight\n",
      "base_model.model.down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_k.lora_A.weight\n",
      "base_model.model.down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_k.lora_B.weight\n",
      "base_model.model.down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_q.lora_A.weight\n",
      "base_model.model.down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_q.lora_B.weight\n",
      "base_model.model.down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_k.lora_A.weight\n",
      "base_model.model.down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_k.lora_B.weight\n",
      "base_model.model.down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_q.lora_A.weight\n",
      "base_model.model.down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_q.lora_B.weight\n",
      "base_model.model.down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_k.lora_A.weight\n",
      "base_model.model.down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_k.lora_B.weight\n",
      "base_model.model.down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_q.lora_A.weight\n",
      "base_model.model.down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_q.lora_B.weight\n",
      "base_model.model.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_k.lora_A.weight\n",
      "base_model.model.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_k.lora_B.weight\n",
      "base_model.model.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_q.lora_A.weight\n",
      "base_model.model.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_q.lora_B.weight\n",
      "base_model.model.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k.lora_A.weight\n",
      "base_model.model.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k.lora_B.weight\n",
      "base_model.model.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_q.lora_A.weight\n",
      "base_model.model.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_q.lora_B.weight\n",
      "base_model.model.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_k.lora_A.weight\n",
      "base_model.model.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_k.lora_B.weight\n",
      "base_model.model.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_q.lora_A.weight\n",
      "base_model.model.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_q.lora_B.weight\n",
      "base_model.model.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k.lora_A.weight\n",
      "base_model.model.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k.lora_B.weight\n",
      "base_model.model.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_q.lora_A.weight\n",
      "base_model.model.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_q.lora_B.weight\n",
      "base_model.model.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_k.lora_A.weight\n",
      "base_model.model.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_k.lora_B.weight\n",
      "base_model.model.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_q.lora_A.weight\n",
      "base_model.model.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_q.lora_B.weight\n",
      "base_model.model.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_k.lora_A.weight\n",
      "base_model.model.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_k.lora_B.weight\n",
      "base_model.model.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_q.lora_A.weight\n",
      "base_model.model.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_q.lora_B.weight\n",
      "base_model.model.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_k.lora_A.weight\n",
      "base_model.model.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_k.lora_B.weight\n",
      "base_model.model.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_q.lora_A.weight\n",
      "base_model.model.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_q.lora_B.weight\n",
      "base_model.model.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_k.lora_A.weight\n",
      "base_model.model.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_k.lora_B.weight\n",
      "base_model.model.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_q.lora_A.weight\n",
      "base_model.model.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_q.lora_B.weight\n",
      "base_model.model.mid_block.attentions.0.transformer_blocks.0.attn1.to_k.lora_A.weight\n",
      "base_model.model.mid_block.attentions.0.transformer_blocks.0.attn1.to_k.lora_B.weight\n",
      "base_model.model.mid_block.attentions.0.transformer_blocks.0.attn1.to_q.lora_A.weight\n",
      "base_model.model.mid_block.attentions.0.transformer_blocks.0.attn1.to_q.lora_B.weight\n",
      "base_model.model.mid_block.attentions.0.transformer_blocks.0.attn2.to_k.lora_A.weight\n",
      "base_model.model.mid_block.attentions.0.transformer_blocks.0.attn2.to_k.lora_B.weight\n",
      "base_model.model.mid_block.attentions.0.transformer_blocks.0.attn2.to_q.lora_A.weight\n",
      "base_model.model.mid_block.attentions.0.transformer_blocks.0.attn2.to_q.lora_B.weight\n",
      "base_model.model.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_k.lora_A.weight\n",
      "base_model.model.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_k.lora_B.weight\n",
      "base_model.model.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_q.lora_A.weight\n",
      "base_model.model.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_q.lora_B.weight\n",
      "base_model.model.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k.lora_A.weight\n",
      "base_model.model.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k.lora_B.weight\n",
      "base_model.model.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_q.lora_A.weight\n",
      "base_model.model.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_q.lora_B.weight\n",
      "base_model.model.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_k.lora_A.weight\n",
      "base_model.model.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_k.lora_B.weight\n",
      "base_model.model.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_q.lora_A.weight\n",
      "base_model.model.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_q.lora_B.weight\n",
      "base_model.model.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k.lora_A.weight\n",
      "base_model.model.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k.lora_B.weight\n",
      "base_model.model.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_q.lora_A.weight\n",
      "base_model.model.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_q.lora_B.weight\n",
      "base_model.model.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_k.lora_A.weight\n",
      "base_model.model.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_k.lora_B.weight\n",
      "base_model.model.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_q.lora_A.weight\n",
      "base_model.model.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_q.lora_B.weight\n",
      "base_model.model.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_k.lora_A.weight\n",
      "base_model.model.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_k.lora_B.weight\n",
      "base_model.model.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_q.lora_A.weight\n",
      "base_model.model.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_q.lora_B.weight\n",
      "base_model.model.up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_k.lora_A.weight\n",
      "base_model.model.up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_k.lora_B.weight\n",
      "base_model.model.up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_q.lora_A.weight\n",
      "base_model.model.up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_q.lora_B.weight\n",
      "base_model.model.up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_k.lora_A.weight\n",
      "base_model.model.up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_k.lora_B.weight\n",
      "base_model.model.up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_q.lora_A.weight\n",
      "base_model.model.up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_q.lora_B.weight\n",
      "base_model.model.up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_k.lora_A.weight\n",
      "base_model.model.up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_k.lora_B.weight\n",
      "base_model.model.up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_q.lora_A.weight\n",
      "base_model.model.up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_q.lora_B.weight\n",
      "base_model.model.up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_k.lora_A.weight\n",
      "base_model.model.up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_k.lora_B.weight\n",
      "base_model.model.up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_q.lora_A.weight\n",
      "base_model.model.up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_q.lora_B.weight\n",
      "base_model.model.up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_k.lora_A.weight\n",
      "base_model.model.up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_k.lora_B.weight\n",
      "base_model.model.up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_q.lora_A.weight\n",
      "base_model.model.up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_q.lora_B.weight\n",
      "base_model.model.up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_k.lora_A.weight\n",
      "base_model.model.up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_k.lora_B.weight\n",
      "base_model.model.up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_q.lora_A.weight\n",
      "base_model.model.up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_q.lora_B.weight\n",
      "base_model.model.up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_k.lora_A.weight\n",
      "base_model.model.up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_k.lora_B.weight\n",
      "base_model.model.up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_q.lora_A.weight\n",
      "base_model.model.up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_q.lora_B.weight\n",
      "base_model.model.up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_k.lora_A.weight\n",
      "base_model.model.up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_k.lora_B.weight\n",
      "base_model.model.up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_q.lora_A.weight\n",
      "base_model.model.up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_q.lora_B.weight\n",
      "base_model.model.up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_k.lora_A.weight\n",
      "base_model.model.up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_k.lora_B.weight\n",
      "base_model.model.up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_q.lora_A.weight\n",
      "base_model.model.up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_q.lora_B.weight\n",
      "base_model.model.up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_k.lora_A.weight\n",
      "base_model.model.up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_k.lora_B.weight\n",
      "base_model.model.up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_q.lora_A.weight\n",
      "base_model.model.up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_q.lora_B.weight\n",
      "base_model.model.up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_k.lora_A.weight\n",
      "base_model.model.up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_k.lora_B.weight\n",
      "base_model.model.up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_q.lora_A.weight\n",
      "base_model.model.up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_q.lora_B.weight\n",
      "base_model.model.up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_k.lora_A.weight\n",
      "base_model.model.up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_k.lora_B.weight\n",
      "base_model.model.up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_q.lora_A.weight\n",
      "base_model.model.up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_q.lora_B.weight\n"
     ]
    }
   ],
   "source": [
    "from safetensors.torch import load_file\n",
    "\n",
    "# Load LoRA weights\n",
    "lora_weights = load_file(\"weights/adapter_model.safetensors\")\n",
    "\n",
    "# ✅ Print all available LoRA layer names\n",
    "print(\"\\n🔹 LoRA Weights Contain These Layers:\\n\")\n",
    "for key in lora_weights.keys():\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "efcbfeef-235b-481b-9e52-ae7a595a0c1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ LoRA weights renamed and saved successfully!\n"
     ]
    }
   ],
   "source": [
    "from safetensors.torch import load_file, save_file\n",
    "\n",
    "# Load LoRA weights\n",
    "lora_weights = load_file(\"weights/adapter_model.safetensors\")\n",
    "\n",
    "# ✅ Rename LoRA layers (remove `.lora_A.weight` and `.lora_B.weight`)\n",
    "new_lora_weights = {}\n",
    "for key in lora_weights.keys():\n",
    "    new_key = key.replace(\".lora_A.weight\", \"\").replace(\".lora_B.weight\", \"\")\n",
    "    new_lora_weights[new_key] = lora_weights[key]\n",
    "\n",
    "# ✅ Save the fixed LoRA weights\n",
    "save_file(new_lora_weights, \"weights/adapter_model_fixed_1.safetensors\")\n",
    "print(\"✅ LoRA weights renamed and saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "00a5b637-9099-40ef-af6c-04a3a679d6da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4b0f114c24548eca9178bcef943ee22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "weights/adapter_model_fixed_1.safetensors does not seem to be in the correct format expected by Custom Diffusion training.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m pipe \u001b[38;5;241m=\u001b[39m StableDiffusionPipeline\u001b[38;5;241m.\u001b[39mfrom_pretrained(base_model, torch_dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat16)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# ✅ Load the Fixed LoRA Weights (No more `lora_A` / `lora_B` issues)\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[43mpipe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_attn_procs\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweights\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43madapter_model_fixed_1.safetensors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# ✅ Merge LoRA into the base model\u001b[39;00m\n\u001b[1;32m     11\u001b[0m pipe\u001b[38;5;241m.\u001b[39msave_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmerged_finetuned_model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/diffusers/loaders/unet.py:225\u001b[0m, in \u001b[0;36mUNet2DConditionLoadersMixin.load_attn_procs\u001b[0;34m(self, pretrained_model_name_or_path_or_dict, **kwargs)\u001b[0m\n\u001b[1;32m    217\u001b[0m     is_model_cpu_offload, is_sequential_cpu_offload \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_lora(\n\u001b[1;32m    218\u001b[0m         state_dict\u001b[38;5;241m=\u001b[39mstate_dict,\n\u001b[1;32m    219\u001b[0m         unet_identifier_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munet_name,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    222\u001b[0m         _pipeline\u001b[38;5;241m=\u001b[39m_pipeline,\n\u001b[1;32m    223\u001b[0m     )\n\u001b[1;32m    224\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 225\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    226\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not seem to be in the correct format expected by Custom Diffusion training.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    227\u001b[0m     )\n\u001b[1;32m    229\u001b[0m \u001b[38;5;66;03m# <Unsafe code\u001b[39;00m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;66;03m# We can be sure that the following works as it just sets attention processors, lora layers and puts all in the same dtype\u001b[39;00m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;66;03m# Now we remove any existing hooks to `_pipeline`.\u001b[39;00m\n\u001b[1;32m    232\u001b[0m \n\u001b[1;32m    233\u001b[0m \u001b[38;5;66;03m# For LoRA, the UNet is already offloaded at this stage as it is handled inside `_process_lora`.\u001b[39;00m\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_custom_diffusion \u001b[38;5;129;01mand\u001b[39;00m _pipeline \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: weights/adapter_model_fixed_1.safetensors does not seem to be in the correct format expected by Custom Diffusion training."
     ]
    }
   ],
   "source": [
    "from diffusers import StableDiffusionPipeline\n",
    "\n",
    "# Load Base Stable Diffusion Model\n",
    "base_model = \"CompVis/stable-diffusion-v1-4\"\n",
    "pipe = StableDiffusionPipeline.from_pretrained(base_model, torch_dtype=torch.float16)\n",
    "\n",
    "# ✅ Load the Fixed LoRA Weights (No more `lora_A` / `lora_B` issues)\n",
    "pipe.unet.load_attn_procs(\"weights\", weight_name=\"adapter_model_fixed_1.safetensors\")\n",
    "\n",
    "# ✅ Merge LoRA into the base model\n",
    "pipe.save_pretrained(\"merged_finetuned_model\")\n",
    "print(\"✅ LoRA successfully merged into the full model! Load from 'merged_finetuned_model'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "517f2b2f-3267-44b0-b51f-48dd0a54bc41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "848de31b10404ae88b52e13d3bf0fc61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ LoRA successfully merged into the full model! Load from 'merged_finetuned_model'\n"
     ]
    }
   ],
   "source": [
    "from diffusers import StableDiffusionPipeline\n",
    "from safetensors.torch import load_file\n",
    "import torch\n",
    "\n",
    "# ✅ Load Base Model (Ensure it matches LoRA training model)\n",
    "base_model = \"CompVis/stable-diffusion-v1-4\"\n",
    "pipe = StableDiffusionPipeline.from_pretrained(base_model, torch_dtype=torch.float16)\n",
    "\n",
    "# ✅ Load LoRA weights\n",
    "lora_weights = load_file(\"weights/adapter_model.safetensors\")\n",
    "\n",
    "# ✅ Merge LoRA into the base model\n",
    "alpha = 1.0  # LoRA scaling factor\n",
    "\n",
    "for key in lora_weights.keys():\n",
    "    if \"lora_A\" in key:\n",
    "        base_key = key.replace(\".lora_A.weight\", \".weight\")  # Match base model layer\n",
    "        key_B = key.replace(\"lora_A\", \"lora_B\")  # Find corresponding lora_B key\n",
    "        \n",
    "        if base_key in pipe.unet.state_dict() and key_B in lora_weights:\n",
    "            A = lora_weights[key]\n",
    "            B = lora_weights[key_B]\n",
    "            merged_weight = pipe.unet.state_dict()[base_key] + alpha * (A @ B)  # Merge LoRA into base weight\n",
    "            \n",
    "            pipe.unet.state_dict()[base_key].copy_(merged_weight)  # Update base model\n",
    "\n",
    "# ✅ Save the fully merged model\n",
    "pipe.save_pretrained(\"merged_finetuned_model\")\n",
    "print(\"✅ LoRA successfully merged into the full model! Load from 'merged_finetuned_model'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "55b40e9c-1a48-4a74-9d41-31d519c0be7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "223f550847d4401c8a68b81658b66ced",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f343ce4c252c46169ac171b651a0c6eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the fully merged model\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\"merged_finetuned_model\", torch_dtype=torch.float16).to(\"mps\")\n",
    "# ✅ Set device to Apple MPS (Metal) instead of CUDA\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(device)\n",
    "pipe.to(device)\n",
    "# Generate Image\n",
    "image = pipe(\"A dirty plate covered in food stains\").images[0]\n",
    "image.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8d7cacc7-ced4-4107-8c13-6ef8b516abaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34a3ae0e11a64be78efa4078936176ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image = pipe(\"A frying pan covered with food stains\").images[0]\n",
    "image.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "34563bec-a5df-4ad9-a29c-269d5bed9a9a",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'StableDiffusionPipelineOutput' object has no attribute 'show'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'StableDiffusionPipelineOutput' object has no attribute 'show'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "33c36e1c-0d38-4041-9ed9-570e3caa3e55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7632966d552e43a1a5fd82c7fe8a9992",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image = pipe(\"dirty dishes in a sink\").images[0]\n",
    "image.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "dfe4a0f9-e1f0-4908-a146-1af7aac30c5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2145e542939d47f584a8594b91b772aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image = pipe(\"dirty glasses\").images[0]\n",
    "image.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fd74f40d-a252-4c66-8dfc-3e1fb17f4303",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc518a9985284f6e981cc8744dbf5559",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image = pipe(\"Glasses with milk stains\").images[0]\n",
    "image.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e160fa65-0a0b-4df7-b801-678db3a8780c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7b20a5905074165915420b9c40bc88c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a994e09e546428eb8e2c2c3be5cb945",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from diffusers import StableDiffusionPipeline\n",
    "import torch\n",
    "\n",
    "# Load Stable Diffusion\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16).to(\"mps\")\n",
    "\n",
    "# Generate synthetic dirty dish images\n",
    "for i in range(1):  \n",
    "    prompt = \"A sink full of dirty dishes covered in food stains, grease, and leftovers, realistic lighting\"\n",
    "    image = pipe(prompt).images[0]\n",
    "    image.save(f\"dirty_dishes_{i}.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b366673d-b249-441d-a8c2-c085b0b61ccd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
